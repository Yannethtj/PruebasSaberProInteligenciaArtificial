{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yannethtj/PruebasSaberProInteligenciaArtificial/blob/main/03-preprocesado_%26_RegresionLogistica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE7-_4DoHVLb",
        "outputId": "2ca416e7-5d14-4624-9101-e9da3ae9fd19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 ./kaggle.json'\n",
            "usage: kaggle [-h] [-v] [-W] {competitions,c,datasets,d,kernels,k,models,m,files,f,config} ...\n",
            "kaggle: error: the following arguments are required: command\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '.'\n",
        "!kaggle\n",
        "!chmod 600 ./kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c udea-ai4eng-20242"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llIC9McpJa_l",
        "outputId": "95834ee7-99f1-4acf-c773-c6b581c3acbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading udea-ai4eng-20242.zip to /content\n",
            "\r  0% 0.00/20.1M [00:00<?, ?B/s]\r 70% 14.0M/20.1M [00:00<00:00, 141MB/s]\n",
            "\r100% 20.1M/20.1M [00:00<00:00, 167MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip udea-ai4eng-20242.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5OPgCMyKU8W",
        "outputId": "e630112b-1df9-4143-9f11-d40783d1e88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  udea-ai4eng-20242.zip\n",
            "  inflating: submission_example.csv  \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "3UTADUWSKZTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "vsdE1IrAKvdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycaret"
      ],
      "metadata": {
        "id": "q-NCAe3QbbtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycaret.classification import setup, compare_models, finalize_model, predict_model\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar datasets\n",
        "\n",
        "# Configurar PyCaret\n",
        "setup(\n",
        "    data=train,\n",
        "    target='RENDIMIENTO_GLOBAL',  # Cambiar 'target' por la columna objetivo\n",
        "    train_size=0.7,\n",
        "    session_id=123,\n",
        "    fold=3,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Comparar modelos\n",
        "mejores_modelos = compare_models(sort='Accuracy', n_select=3, cross_validation=False)\n",
        "\n",
        "# Finalizar y predecir con el mejor modelo\n",
        "mejor_modelo = finalize_model(mejores_modelos[0])\n",
        "predicciones = predict_model(mejor_modelo, data=test)\n",
        "\n",
        "# Ver predicciones\n",
        "print(predicciones)\n"
      ],
      "metadata": {
        "id": "leOVeOvza5og",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos Pycaret para ver cuales serian los mejores modelos para nuestro dataset.\n",
        "<br/>\n",
        "y despues de aprox 20 minutos nos presentó que los mejores modelos son xgboost, lightgbm los cuales usaremos para nuestros proximos modelos."
      ],
      "metadata": {
        "id": "lxz0PBMY_n-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "jES1V0t6WAdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicialmente vamos a tratar las columnas que quedamos por organizar en mi preprocesado, primero vamos a usar una función para normalizar el texto y que ya no aparezcan caracteres especiales.\n",
        "Despues vamos agrupar los valores siguiendo su frecuencia relativa"
      ],
      "metadata": {
        "id": "IT99tO4DXPYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def limpiar_y_preprocesar(data):\n",
        "  # Label encoding\n",
        "  label_encoder = LabelEncoder()\n",
        "  new_labels = pd.DataFrame(label_encoder.fit_transform(data['RENDIMIENTO_GLOBAL']), columns=['RENDIMIENTO_GLOBAL'])\n",
        "  data = pd.concat([data.drop('RENDIMIENTO_GLOBAL', axis=1), new_labels], axis=1)\n",
        "  data = data.drop('ID', axis=1)\n",
        "  data['PERIODO'] = data['PERIODO'].astype(str).apply(lambda text: text[:4])\n",
        "  def normalize_text(text: str):\n",
        "    import unicodedata\n",
        "    import re\n",
        "    if not pd.isna(text):\n",
        "        # Normaliza el texto eliminando acentos\n",
        "        normalized = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "        # Elimina caracteres no deseados, incluidos \"¿\" y \"?\"\n",
        "        return re.sub(r'[¿?]', '', normalized)\n",
        "    return text\n",
        "\n",
        "  def group_values_by_frequency(data: pd.DataFrame, col: str, thresholds: list[float] = [0.05, 0.2]):\n",
        "    \"\"\"\n",
        "    Agrupa los valores de una columna en función de su frecuencia relativa.\n",
        "    Los valores se agrupan en categorías basadas en una lista de umbrales.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): DataFrame de entrada.\n",
        "        col (str): Nombre de la columna a procesar.\n",
        "        thresholds (list[float]): Lista de umbrales para definir los grupos.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: Serie con los valores agrupados.\n",
        "    \"\"\"\n",
        "    porcentaje = data[col].value_counts(normalize=True)\n",
        "\n",
        "    def asignar_grupo(value):\n",
        "        freq = porcentaje[value]\n",
        "        for i, t in enumerate(thresholds):\n",
        "            if freq < t:\n",
        "                return f'GRUPO_{i+1}'\n",
        "        return 'GRUPO_ALTO'\n",
        "\n",
        "    return data[col].apply(asignar_grupo)\n",
        "\n",
        "\n",
        "  for col in ['ESTU_PRGM_ACADEMICO', 'ESTU_PRGM_DEPARTAMENTO']:\n",
        "      data[col] = data[col].apply(normalize_text)\n",
        "  for col in ['ESTU_PRGM_ACADEMICO', 'ESTU_PRGM_DEPARTAMENTO']:\n",
        "      data[col] = group_values_by_frequency(data, col)\n",
        "\n",
        "  mapeo = {'Si': 1, 'S': 1, 'No': 0, 'N':0}\n",
        "  target_columns = ['FAMI_TIENEINTERNET', 'ESTU_PAGOMATRICULAPROPIO']\n",
        "  for column in target_columns:\n",
        "      data[column] = data[column].map(mapeo)\n",
        "  for col in target_columns:\n",
        "    data[col] = data[col].fillna(0)\n",
        "\n",
        "  mapeo_estrato = {'Sin Estrato': 0, 'Estrato 1': 1, 'Estrato 2': 2, 'Estrato 3': 3, 'Estrato 4': 4, 'Estrato 5': 5, 'Estrato 6': 6}\n",
        "\n",
        "  data['FAMI_ESTRATOVIVIENDA'] = data['FAMI_ESTRATOVIVIENDA'].map(mapeo_estrato)\n",
        "\n",
        "  mode_value = data['FAMI_ESTRATOVIVIENDA'].mode()[0]\n",
        "\n",
        "\n",
        "  data['FAMI_ESTRATOVIVIENDA'] = data['FAMI_ESTRATOVIVIENDA'].fillna(mode_value)\n",
        "\n",
        "  variables_continuas = ['ESTU_VALORMATRICULAUNIVERSIDAD', 'ESTU_HORASSEMANATRABAJA']\n",
        "\n",
        "  mapeo_valores_matricula = {\n",
        "      'Menos de 500 mil': 0,\n",
        "      'Entre 500 mil y menos de 1 millón': 1,\n",
        "      'Entre 1 millón y menos de 2.5 millones': 2,\n",
        "      'Entre 2.5 millones y menos de 4 millones': 3,\n",
        "      'Entre 4 millones y menos de 5.5 millones': 4,\n",
        "      'Entre 5.5 millones y menos de 7 millones': 5,\n",
        "      'Más de 7 millones': 6,\n",
        "      'No pagó matrícula': 7\n",
        "  }\n",
        "\n",
        "  mapeo_valores_horas_trabajo = {\n",
        "      '0': 0,\n",
        "      'Menos de 10 horas': 1,\n",
        "      'Entre 11 y 20 horas': 2,\n",
        "      'Entre 21 y 30 horas': 3,\n",
        "      'Más de 30 horas': 4\n",
        "  }\n",
        "\n",
        "\n",
        "  data['ESTU_VALORMATRICULAUNIVERSIDAD'] = data['ESTU_VALORMATRICULAUNIVERSIDAD'].map(mapeo_valores_matricula)\n",
        "  data['ESTU_HORASSEMANATRABAJA'] = data['ESTU_HORASSEMANATRABAJA'].map(mapeo_valores_horas_trabajo)\n",
        "  for col in variables_continuas:\n",
        "    data[col] = data[col].fillna(0)\n",
        "\n",
        "  moda_padre = data['FAMI_EDUCACIONPADRE'].mode()[0]\n",
        "  moda_madre = data['FAMI_EDUCACIONMADRE'].mode()[0]\n",
        "\n",
        "\n",
        "  data['FAMI_EDUCACIONPADRE'] = data['FAMI_EDUCACIONPADRE'].fillna(moda_padre)\n",
        "  data['FAMI_EDUCACIONMADRE'] = data['FAMI_EDUCACIONMADRE'].fillna(moda_madre)\n",
        "  data = pd.get_dummies(data, columns=[ 'FAMI_EDUCACIONPADRE', 'FAMI_EDUCACIONMADRE','ESTU_PRGM_ACADEMICO', 'ESTU_PRGM_DEPARTAMENTO'], prefix=['FAMI_EDUCACIONPADRE', 'FAMI_EDUCACIONMADRE','ESTU_PRGM_ACADEMICO', 'ESTU_PRGM_DEPARTAMENTO'])\n",
        "  data.replace({True: 1, False: 0})\n",
        "\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "TV1xozwn7tiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtr = train\n",
        "dts = test\n",
        "lentr = len(dtr)\n",
        "dtr.shape, dts.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Flq-yGpQWUjP",
        "outputId": "538fd113-e2cd-4883-df86-f84654e46bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((692500, 12), (296786, 12))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_cols = [i for i in dtr.columns if i!=\"RENDIMIENTO_GLOBAL\"]\n",
        "all_data = pd.concat((dtr[source_cols], dts[source_cols]))\n",
        "all_data.index = range(len(all_data))\n",
        "all_data = limpiar_y_preprocesar(all_data)\n",
        "\n",
        "Xtrk, ytrk = all_data.iloc[:lentr].values, dtr[\"RENDIMIENTO_GLOBAL\"].values\n",
        "Xtsk  = all_data.iloc[lentr:].values\n",
        "\n",
        "print (Xtrk.shape, ytrk.shape)\n",
        "print (Xtsk.shape)"
      ],
      "metadata": {
        "id": "d7LUho6OWW73",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(len(Xtrk)*0.7)\n",
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_XU3jEuWYHf",
        "outputId": "64ed53e6-1801-4841-8338-64ccfa3fc498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "484749"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idxs = np.random.permutation(len(Xtrk))\n",
        "idxs_trm = idxs[:n]\n",
        "idxs_tsm = idxs[n:]\n",
        "\n",
        "Xtrm = Xtrk[idxs_trm]\n",
        "ytrm = ytrk[idxs_trm]\n",
        "\n",
        "Xtsm = Xtrk[idxs_tsm]\n",
        "ytsm = ytrk[idxs_tsm]\n",
        "\n",
        "print (Xtrm.shape, ytrm.shape, Xtsm.shape, ytsm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njXA_RweWZNO",
        "outputId": "114bdef6-99f3-4110-b0ba-b5ade8fd4192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(484749, 35) (484749,) (207751, 35) (207751,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El primer modelo usado es la RegresionLogistica, aunque no es la mejor a través de los resultados de pycarent, sí es rapido de correr y podemos tener un modelo base."
      ],
      "metadata": {
        "id": "wYiK3mnnBmTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "lr = LogisticRegression(max_iter=100, n_jobs=-1)\n",
        "scores = cross_val_score(lr, Xtrk, ytrk, cv=5)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juXi1HAtXDff",
        "outputId": "6f811828-d584-489a-ca4d-36ac27f6c097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.36581227 0.35916245 0.36239711 0.3620722  0.36238989]\n"
          ]
        }
      ]
    }
  ]
}